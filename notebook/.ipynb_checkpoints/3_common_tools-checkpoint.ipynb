{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 解析cifar-10数据集\n",
    "运行J_SENet/src/01_parse_cifar10_to_png.py，将cifar-10数据集解析为png格式   \n",
    "\n",
    "数据存放位置：   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-python.tar   \n",
    "解压得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-batches-py\n",
    "\n",
    "经过01_parse_cifar10_to_png.py，得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_train   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_test  \n",
    "\n",
    "\n",
    "### 数据展示\n",
    "<img src=\"imgs/cifar10.png\" width=\"700\" heith=\"700\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     23,
     56
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tools.cifar10_dataset import CifarDataset\n",
    "from tools.common_tools import ModelTrainer, show_confMat, plot_line\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        assert (os.path.exists(data_dir)), \"data_dir:{} 不存在！\".format(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self._get_img_info()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.img_info[index]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.img_info) == 0:\n",
    "            raise Exception(\"未获取任何图片路径，请检查dataset及文件路径！\")\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def _get_img_info(self):\n",
    "        sub_dir_ = [name for name in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, name))]\n",
    "        sub_dir = [os.path.join(self.data_dir, c) for c in sub_dir_]\n",
    "\n",
    "        self.img_info = []\n",
    "        for c_dir in sub_dir:\n",
    "            path_img = [(os.path.join(c_dir, i), int(os.path.basename(c_dir))) for i in os.listdir(c_dir) if\n",
    "                        i.endswith(\"png\")]\n",
    "            self.img_info.extend(path_img)\n",
    "            \n",
    "            \n",
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = np.array(img_) * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     3,
     11
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 5.3725e-01,  ..., 4.5882e-01,\n",
      "          4.7451e-01, 5.7647e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.3725e-01,  ..., 4.0000e-01,\n",
      "          4.5882e-01, 5.8039e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.4118e-01,  ..., 4.4314e-01,\n",
      "          5.2941e-01, 5.8824e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.9802e-08, 2.9802e-08, 5.8824e-01,  ..., 5.2549e-01,\n",
      "          5.3333e-01, 6.2745e-01],\n",
      "         [2.9802e-08, 2.9802e-08, 5.8824e-01,  ..., 4.5882e-01,\n",
      "          5.2549e-01, 6.3137e-01],\n",
      "         [2.9802e-08, 2.9802e-08, 5.9216e-01,  ..., 4.8235e-01,\n",
      "          6.0392e-01, 6.3922e-01],\n",
      "         ...,\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 6.1961e-01,  ..., 5.5294e-01,\n",
      "          5.6471e-01, 6.5882e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.1961e-01,  ..., 4.8235e-01,\n",
      "          5.4902e-01, 6.6275e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.2353e-01,  ..., 4.9804e-01,\n",
      "          6.1176e-01, 6.7059e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]) 0\n",
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x2046037C9B0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2047d54ed68>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_dir = os.path.join(BASE_DIR, \"..\", \"Data\", \"cifar-10\",  \"cifar10_train\")\n",
    "train_data = CifarDataset(data_dir=train_dir, transform=train_transform)\n",
    "\n",
    "print(train_data.__len__()) \n",
    "\n",
    "img_tensor, label = train_data.__getitem__(66)\n",
    "\n",
    "img_rgb = transform_invert(img_tensor, train_transform)\n",
    "print(img_tensor, label)\n",
    "print(img_rgb)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (dense1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans1): Transition(\n",
      "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans2): Transition(\n",
      "    (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=132, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "densenet_model = DenseNet(growthRate=12, depth=40, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "print(densenet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型训练器  ModelTrainer\n",
    "\n",
    "定义模型训练类，用于完成模型前向，反向传播，并记录训练loss，accuracy等指标  \n",
    "\n",
    "目的是简化主代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     4,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, loss_f, optimizer, epoch_id, device, max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))   # 混淆矩阵，用于绘图，且计算accuracy，precision，recall等指标很方便\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_f(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())                  # 记录每个iterations的loss，待会取均值就得到epochs的loss\n",
    "            acc_avg = conf_mat.trace() / conf_mat.sum()     # 利用混淆矩阵求取accuracy， 矩阵的迹 除以 总元素 \n",
    "\n",
    "            # 每10个iteration 打印一次训练信息，loss为10个iteration的平均\n",
    "            if i % 50 == 50 - 1:\n",
    "                print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                    epoch_id + 1, max_epoch, i + 1, len(data_loader), np.mean(loss_sigma), acc_avg))\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def valid(data_loader, model, loss_f, device):\n",
    "        model.eval()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())\n",
    "\n",
    "        acc_avg = conf_mat.trace() / conf_mat.sum()\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.混淆矩阵概念\n",
    "混淆矩阵(Confusion Matrix)常用来观察分类结果，其是一个N\\*N的方阵，N表示类别数。 \n",
    "\n",
    "混淆矩阵的行表示真实类别，列表示预测类别。例如，猫狗的二分类问题，有猫的图像10张，狗的图像30张，模型对这40张图片进行预测，得到的混淆矩阵为\n",
    "\n",
    "| 类别|  阿猫   | 阿狗  |\n",
    "|----|  ----  | ----  |\n",
    "|阿猫 | 7  | 3 |\n",
    "|阿狗| 10  | 20 |\n",
    "\n",
    "\n",
    "从第一行中可知道，10张猫的图像中，7张预测为猫，3张预测为狗，猫的召回率(Recall)为7/10 = 70%，   \n",
    "从第二行中可知道，30张狗的图像中，8张预测为猫，22张预测为狗，狗的召回率为20/30 = 66.7%，  \n",
    "从第一列中可知道，预测为猫的17张图像中，有7张是真正的猫，猫的精确度(Precision)为7 / 17 = 41.17%   \n",
    "从第二列中可知道，预测为狗的23张图像中，有20张是真正的狗，狗的精确度(Precision)为20 / 23 = 86.96%  \n",
    "\n",
    "模型的准确率(Accuracy)为  (7+20) / 40 = 67.5%   \n",
    "\n",
    "可以发现通过混淆矩阵可以清晰的看出网络模型的分类情况，若再结合上颜色可视化，可方便的看出模型的分类偏好。  \n",
    "\n",
    "\n",
    "<img src=\"imgs/Confusion_Matrixtrain.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  CifarSEBasicBlock\n",
    "针对Cifar-10的 Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CifarSEBasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, reduction=16):\n",
    "        super(CifarSEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        if inplanes != planes:\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                                            nn.BatchNorm2d(planes))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)   ###### SE \n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_14",
   "language": "python",
   "name": "pytorch_14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
